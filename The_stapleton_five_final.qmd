---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: London Airbnb Rental Market Report --- Based on the Analysis of STL
author: The Stapleton Five
execute:
  echo: false
  warning: false
  freeze: true
format:
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python (base)
    language: python
    name: base
---

## Declaration of Authorship {.unnumbered .unlisted}

We, [The Stapleton Five], pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date:16/12/2024

Student Numbers: 24107824, 24057654, 24054682, 24070831, 24153654

## Brief Group Reflection

| What Went Well | What Was Challenging |
| -------------- | -------------------- |
| A Literature collection and review were smooth. We gained a good understanding of Insitebnb from the literature and identified appropriate references to support our arguments.             | B Initially, dynamic maps failed to render successfully.                   |
| C Successfully applied statistical and spatial analysis methods.             | D It was challenging to determine a clear theme amidst a large volume of figures and maps.                   |

## Priorities for Feedback

1. Whether our written explanations are clear and the structure is appropriate, and suggestions on areas for improvement.
2. Whether each analytical method used in the code has been correctly applied.
{{< pagebreak >}}

```{python}
%matplotlib inline
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt  
import seaborn as sns
import geopandas as gpd
import requests
from requests import get
from urllib.parse import urlparse
from functools import wraps
import matplotlib
```

```{python}
#| include: false
#Refer to Practical 10 "https://jreades.github.io/fsds/practicals/Practical-10-Presenting_Data.html"
def cache_che(func):
    @wraps(func)
    def wrapper(url_p, dest):
        url=urlparse(url_p)
        file_name=os.path.split(url.path)[-1]
        dest_name=os.path.join(dest, file_name)
        if os.path.isfile(dest_name):
            print(f'{dest_name} have been found locally')
            return(dest_name)
        else:
            print(f'{dest_name} isn\'t here, it is downlowded now')
            return(func(url_p, dest_name))
    return wrapper
@cache_che
def data_cache(url_p:str, dest:str) -> str:
    path = os.path.split(dest)[0]
    if path != '':
        os.makedirs(path, exist_ok=True)
    with open(dest, "wb") as file: 
        response = get(url_p)
        file.write(response.content) 
    return dest
```

```{python}
#| include: false
host = 'https://orca.casa.ucl.ac.uk'
path = '~jreades/data'
file = '20240614-London-listings.csv.gz'
url  = f'{host}/{path}/{file}'

if os.path.exists(file):
  df = pd.read_csv(file, compression='gzip', low_memory=False)
else: 
  df = pd.read_csv(url, compression='gzip', low_memory=False)
  df.to_csv(file, compression='gzip', index=False)
```

```{python}
#| include: false
#extract unique host id
unique_id = df['host_id'].dropna().unique()
df1 = df[df["host_id"].isin(unique_id)]
```

```{python}
#| include: false
#replace $ with blankspace
df['price']=df['price'].str.replace('$','',regex=False).str.replace(',', '', regex=False).astype(float) #convert price into float
```

```{python}
#| include: false
#use request-url to read boro
url = 'https://github.com/jreades/i2p/blob/master/data/src/Boroughs.gpkg?raw=true'
file_name='Boroughs.gpkg'
with open(file_name, "wb") as file: 
    response = get(url)
    file.write(response.content) 
boroughs = gpd.read_file(file_name)
```

```{python}
#| include: false
#use function to read boros 
path = 'https://github.com/jreades/fsds/blob/master/data/src/' 
dest = 'Data'
boroughs = gpd.read_file(data_cache(path+'Boroughs.gpkg?raw=true', dest))
gdf_bnb = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude, crs='epsg:4326'))
bnb_gdf = gdf_bnb.to_crs(boroughs.crs)
```

```{python}
#| include: false
boroughs_json = boroughs  
bnb_gdf = bnb_gdf 
room_types = bnb_gdf['room_type'].unique()
colors = ['#A23B72', '#6CB4EE', '#3D315B', '#F2C641']
room_color_map = dict(zip(room_types, colors))
fig, ax = plt.subplots(1, 1, figsize=(12, 10))
boroughs_json.plot(ax=ax, color='lightgray', edgecolor='white', linewidth=0.5)
for room_type in room_types:
    subset = bnb_gdf[bnb_gdf['room_type'] == room_type]
    subset.plot(
        ax=ax,
        markersize=5,
        color=room_color_map[room_type],
        label=room_type,
        alpha=0.7
    )
plt.title("Airbnb Listings by Room Type in London", fontsize=15)
plt.legend(title="Room Type", fontsize=10, title_fontsize=12, loc='upper left')
ax.axis('off')
plt.tight_layout()
plt.show()
```

```{python}
#| include: false
# Distribution of listings by boroughs and its median price
bnb_borough=gpd.sjoin(bnb_gdf, boroughs, how="inner", predicate="within")
bnb_borough.drop(index=bnb_borough[bnb_borough.NAME.isna()].index, axis=1, inplace=True)
bnb_counts_price=bnb_borough.groupby("NAME").agg(count=('NAME', 'count'), median=('price', 'median')).reset_index()
bnb_merge=pd.merge(boroughs, bnb_counts_price, on="NAME")
f, ax=plt.subplots(1,2,gridspec_kw={'width_ratios':[6,6]}, figsize=(12,8))
bnb_merge.plot(ax=ax[0],column='median',legend=True,cmap='plasma')
ax[0].set_title("Median Price", size=18)
bnb_merge.plot(ax=ax[1],column='count',legend=True,cmap='viridis')
ax[1].set_title("Count", size=18)
for x in ax:
    x.set_xticks([]) 
    x.set_yticks([])
    x.set_facecolor((0.8, 0.9, 1, 1))
    x.spines['left'].set_visible(False)
    x.spines['right'].set_visible(False)
    x.spines['top'].set_visible(False)
    x.spines['bottom'].set_visible(False)
    x.grid(which='major', visible=True, axis='both', color='lightgrey', linestyle='-', linewidth=0.5, zorder=0)
    x.set_axisbelow(True)
f.suptitle('Distribution of Listings and its Price by Boroughs', x=0.025, ha='left', size=22)
plt.figtext(x=0.025, y=0.88, s=f"Total listings: {bnb_merge['count'].sum()}", size=14)
```

```{python}
#| include: false
listings_type = df.loc[:, ['room_type', 'latitude', 'longitude', 'number_of_reviews', 'price']]
listings = listings_type.groupby('room_type').agg(
    count=('room_type', 'count'),
    price=('price', 'median')
).reset_index()

room_types = listings['room_type']
room_counts = listings['count']
median_prices = listings['price']

listings['proportion'] = listings['count'] / listings['count'].sum()

fig, ax1 = plt.subplots(figsize=(10, 6))
bars = ax1.bar(room_types, room_counts, color='lightblue', label='Number of Listings')
for bar in bars:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width() / 2, height, f'{int(height)}', ha='center', va='bottom')

ax2 = ax1.twinx()
ax2.plot(room_types, median_prices, color='green', marker='s', linestyle='--', label='Median Price')
ax1.set_xlabel("Room Type")
ax1.set_ylabel("Number of Listings", color='blue')
ax2.set_ylabel("Median Price ($)", color='green')
plt.title("Median Price by Room Type in London")
fig.legend(loc='upper left', bbox_to_anchor=(0.7, 0.85))
plt.tight_layout()
plt.show()
for index, row in listings.iterrows():
    print(f"{row['room_type']}: {row['proportion']:.2%}")
```

```{python}
#| include: false
listings_type = df.loc[:, ['room_type','latitude', 'longitude','number_of_reviews']]
counts = listings_type['room_type'].value_counts().to_dict()
orders = ['Entire home/apt', 'Private room', 'Shared room', 'Hotel room']
plt.figure(figsize=(8,6))
ax=sns.countplot(data=listings_type, x='room_type', palette="dark", hue=listings_type['room_type'])
for c in ax.containers:
    labels = [counts[l] for l in counts.keys()]
    ax.bar_label(c, label=labels, fontsize=10)
plt.title('Counts of Different Room Type in London')
plt.show()
```

```{python}
#| include: false
#word cloud
import nltk
import re
from nltk.tokenize import word_tokenize, sent_tokenize 
from nltk.tokenize.toktok import ToktokTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.feature_extraction.text import CountVectorizer 
from sklearn.decomposition import LatentDirichletAllocation
from nltk import ngrams, FreqDist
from wordcloud import WordCloud
```

```{python}
#| include: false
#word cloud about description of listings
description = df.loc[:, 'description']
pattern = re.compile(r'<.*?>')
description_na = description.fillna(' ').values
description_norm = []
for t in description_na:
    clean_text = re.sub(pattern, ' ', t)
    description_norm.append(clean_text)
tfvectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1,3), max_df=0.75, min_df=0.01, stop_words='english')
tfdescription = tfvectorizer.fit_transform(description_norm)
tfdif_des = pd.DataFrame(data=tfdescription.toarray(),columns=tfvectorizer.get_feature_names_out())
tfdif_des.sum().sort_values(ascending=False)
f,ax = plt.subplots(1,1,figsize=(8, 8))
plt.gcf().set_dpi(200)
cloud_des=WordCloud(background_color='white',max_words=120).generate_from_frequencies(tfdif_des.sum())
ax.imshow(cloud_des) 
ax.axis("off")
```

```{python}
#| include: false
#word cloud about host info
host_about = df1.loc[:, 'host_about']
pattern = re.compile(r'<.*?>')
host_about_na = host_about.fillna(' ').values
tfvectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1,3), max_df=0.75, min_df=0.01, stop_words='english')
tfdescription = tfvectorizer.fit_transform(host_about_na)
tfdif_des = pd.DataFrame(data=tfdescription.toarray(),columns=tfvectorizer.get_feature_names_out())
tfdif_des.sum().sort_values(ascending=False)
f,ax = plt.subplots(1,1,figsize=(8, 8))
plt.gcf().set_dpi(200)
cloud_des=WordCloud(background_color='white',max_words=120).generate_from_frequencies(tfdif_des.sum())
ax.imshow(cloud_des) 
ax.axis("off")
```

```{python}
#| include: false
property1 = df.property_type.value_counts()
sorted1 = property1.sort_values(ascending=False).reset_index()
sorted_min = sorted1["count"].min()
sorted_min_type = sorted1[sorted1["count"]==sorted_min]['property_type']
print(f"There is a total of {sorted1.shape[0]} property types, and {sorted1.loc[sorted1['count'].idxmax(), 'property_type']} is the most, which has {sorted1['count'].max()}, {sorted_min_type.tolist()} are the least")
```

```{python}
#| include: false
df1["host_response_rate"] = df1["host_response_rate"].str.replace('%', '').astype(float) / 100
df1["host_acceptance_rate"] = df1["host_acceptance_rate"].str.replace('%', '').astype(float) / 100
response_rate_mean = df1["host_response_rate"].dropna().mean()
acceptance_rate_mean = df1["host_acceptance_rate"].dropna().mean()
print(f"The average acceptance rate of host in London is {acceptance_rate_mean}, and average response rate is {response_rate_mean}")
response_time = df1["host_response_time"].dropna().value_counts()
print(f"{response_time['within an hour']} Landlord replies within an hour, {response_time['within a few hours']} landlord replies within a few hours, {response_time['within a day']} landlord replies within a day, {response_time['a few days or more']} landlord replies only after a few days or more")
```

```{python}
#| include: false
#three aspects about whether hosts have profiles and whether they are superhosts, and whether their identities are verified
identity = df1["host_identity_verified"].dropna().value_counts()
profile = df1["host_has_profile_pic"].dropna().value_counts()
superhost = df1["host_is_superhost"].dropna().value_counts()
f, ax = plt.subplots(figsize=(8,6))
bars1 = identity.plot(position=0, ax=ax, kind="bar", color="grey", width=0.1, label='Counts of T/F Identity Verified')
bars2 = profile.plot(position=1, ax=ax, kind="bar", color="skyblue", width=0.1, label='Counts of T/F Have Profiles')
bars3 = superhost.plot(position=2, ax=ax, kind="bar", color="green", width=0.1, label='Counts of T/F Superhost')
ax.set_ylabel("Counts")
ax.set_xlabel("T/F Identity Verified/Have Profiles/Superhost")
ax.set_xticklabels(['TRUE', 'FALSE'], rotation=45)
plt.legend()
plt.show()
```

```{python}
#| include: false
#the date when the host publish his listing
date = df1["host_since"].dropna()
date = pd.to_datetime(date, format='mixed')
date_df = pd.DataFrame({"dates":date})
year = date_df['dates'].dt.year
year = date_df['dates'].dt.year
year_counts = year.value_counts()
year_counts_df = pd.DataFrame(year_counts).reset_index()
year_counts_df.sort_values(by='dates', ascending=False, inplace=True)
f, ax = plt.subplots(figsize=(8,6))
year_counts_df.plot(kind='bar', ax=ax, x="dates", y="count", color="indigo", legend=False)
ax.set_title("Counts of Host Years")
plt.xticks(rotation=45)
ax.set_xlabel('Year')
ax.set_ylabel('Frequency')
ax.legend()
plt.show()
```

```{python}
#| include: false
# counts of whose listings are more than 100 in inside airbnb data
# only in the scraped data of inside airbnb 
host = df[["host_name", "host_id"]]
host_counts = host.groupby("host_id").size().sort_values(ascending=False).reset_index()
host_counts = host_counts.rename(columns={0:"count"})
host_id = host_counts[host_counts["count"] >= 100]
id_host = host_id["host_id"]
host_filtered = host[host["host_id"].isin(id_host)][["host_name", "host_id"]].drop_duplicates()
print(f"There are {host_filtered.shape[0]} hosts whose listings are more than 100 in Inside Airbnb Data")
total=df1["host_total_listings_count"]
listings_total = total.value_counts().reset_index()
print(f'Landlord with the most properties owns {int(listings_total["host_total_listings_count"].max())} properties, and there are {listings_total["count"].max()} hosts which have one listing')
host_counts_100=host_counts[host_counts["count"]>=100]
host_100 = pd.merge(host_counts_100, host_filtered, on="host_id")
f, ax= plt.subplots(figsize=(8,6))
host_100.plot(ax=ax, kind="bar", x="host_name", y="count", color="turquoise")
ax.set_title("Hosts Whose Listings are More Than 100")
ax.set_xlabel("Name")
ax.set_ylabel("Count")
plt.show()
```

```{python}
#| include: false
# Filter hosts with more than 100 short-term rental listings
short_term_rentals = df[(df["minimum_nights"] <= 90) & (df["availability_90"] > 0)]
host = short_term_rentals[["host_name", "host_id", "calculated_host_listings_count"]]
host_counts = host.groupby("host_id")["calculated_host_listings_count"].max().reset_index()
host_counts = host_counts.sort_values(by="calculated_host_listings_count", ascending=False)
hosts_over_100 = host_counts[host_counts["calculated_host_listings_count"] >= 100]
hosts_filtered = pd.merge(hosts_over_100, short_term_rentals[["host_id", "host_name"]], on="host_id").drop_duplicates()
#print(f"There are {hosts_filtered.shape[0]} hosts with more than 100 short-term listings in the dataset.")
f, ax = plt.subplots(figsize=(8, 6))
hosts_filtered.plot(ax=ax, kind="bar", x="host_name", y="calculated_host_listings_count", color="SkyBlue")
ax.set_title("Hosts With More Than 100 STL")
ax.set_xlabel("Host Name")
ax.set_ylabel("Number of Short-Term Listings")
plt.tight_layout()
plt.show()
```

```{python}
#| include: false
verifications_h = df1['host_verifications'].dropna().value_counts().reset_index()
verifications_h.head()
print(f"Most hosts use {verifications_h['host_verifications'].values[0]} as methods of verifications which is as high as {verifications_h['count'].max()}, least hosts use {verifications_h.loc[verifications_h['host_verifications'].idxmin(), 'host_verifications']} as ways of verification which is as low as {verifications_h['count'].min()}")
```

```{python}
#| include: false
max_night = df[df['maximum_nights']<2000]['maximum_nights'].value_counts().reset_index()
max_night.head()
print(f'The most frequent number of maximum nights in these listings is {max_night["maximum_nights"][0]}, which is as high as {max_night["count"].max()}')
```

```{python}
#| include: false
#group by 'property_type','room_type' from practical 9
agg_df = df[(df['price']<=2000) & (df['number_of_reviews']>0)][['price', 'property_type','room_type','number_of_reviews']]
agg = agg_df.groupby(by=['property_type','room_type']).agg(counts = ("property_type", "count"), median_price = ("price", "median")).reset_index().sort_values(by=['counts','room_type','property_type'], ascending=[False,True,True])
agg.head(20)
```

```{python}
#| include: false
# Unless planning permission is obtained, Londoners are restricted to renting their property short term for a maximum of 90 nights in a calendar year.
# from https://www.london.gov.uk/programmes-strategies/housing-and-land/improving-private-rented-sector/guidance-short-term-and-holiday-lets-london
# Next is STL-related analysis
bnb_stl = bnb_gdf[bnb_gdf['minimum_nights']<=90][['price', 'room_type', 'property_type', 'minimum_nights', 'geometry', 'number_of_reviews','availability_365']]
stl_borough=gpd.sjoin(bnb_stl, boroughs, how="inner", predicate="within")
stl_counts=stl_borough.groupby("NAME").size().reset_index()
stl_counts=stl_counts.rename(columns={0:"counts"})
stl_merge=pd.merge(boroughs, stl_counts, on="NAME")
f, ax=plt.subplots(figsize=(12,8))
ax=stl_merge.plot(ax=ax, column="counts", cmap="viridis", edgecolor="black", legend=True)
plt.title("Airbnb STL Listings Distribution in Boroughs")
plt.show()
```

```{python}
#| include: false
stl = df[df['minimum_nights']<=90][['price', 'room_type', 'property_type', 'minimum_nights']]
stl_agg = stl.groupby(by=['property_type','room_type']).agg(counts = ("property_type", "count"), mean_price = ("price", "mean")).reset_index().sort_values(by=['counts','room_type','property_type'], ascending=[False,True,True])
stl_agg.head()
```

```{python}
#| include: false
stl_rtype = stl['room_type'].value_counts()
stl_rtype.head()
stl_ptype = stl['property_type'].value_counts().reset_index()
stl_ptype_7 = stl_ptype[:7]
other_types = pd.DataFrame({"property_type":['Others'], "count":[stl_ptype['count'][7:].sum()]})
stl_ptype2 = pd.concat([stl_ptype_7, other_types], ignore_index=True)
pastel_cmap = plt.get_cmap('Pastel1', len(stl_ptype2['count']))
colors = [pastel_cmap(i) for i in range(len(stl_ptype2['count']))]
plt.figure(figsize=(8, 6))
plt.pie(stl_ptype2['count'], 
        labels=stl_ptype2['property_type'], 
        autopct='%1.1f%%', 
        startangle=90, 
        colors=colors) 
plt.title("STL Property Type Pie Chart")
plt.show()
```

```{python}
#| include: false
stl_sh = df[df['minimum_nights']<=90][['host_is_superhost', 'host_total_listings_count']]
stl_sh_count = stl_sh.groupby('host_is_superhost').agg(count_sh=('host_is_superhost', 'count'), mean_count=('host_total_listings_count', 'mean')).reset_index()
print(stl_sh_count.head())
print(f'There are a total of listings of {stl.shape[0]} STL listings in London')
```

```{python}
#| include: false
#read msoa-level dara using function
MSOA = gpd.read_file(data_cache('http://orca.casa.ucl.ac.uk/~jreades/data/MSOA-2011.gpkg', dest))
MSOA = MSOA.to_crs(epsg=27700)
MSOA['area_km2'] = MSOA['geometry'].area/1e6
```

```{python}
#| include: false
#msoa-level msoa 
bnb_stl = bnb_gdf[bnb_gdf['minimum_nights']<=90][['price', 'room_type', 'property_type', 'minimum_nights', 'geometry', 'number_of_reviews']]
stl_msoa = gpd.sjoin(bnb_stl, MSOA.drop(columns=['MSOA11NM', 'LAD11CD', 'LAD11NM', 'RGN11CD', 'RGN11NM','USUALRES', 'HHOLDRES', 'COMESTRES', 'POPDEN', 'HHOLDS', 'AVHHOLDSZ']), how="inner", predicate="within")
stl_counts_msoa = stl_msoa.groupby("MSOA11CD").size().reset_index()
stl_counts_msoa = stl_counts_msoa.rename(columns={0:"counts"})
stl_merge_msoa = pd.merge(MSOA.drop(columns=['MSOA11NM', 'LAD11CD', 'LAD11NM', 'RGN11CD', 'RGN11NM','USUALRES', 'HHOLDRES', 'COMESTRES', 'POPDEN', 'HHOLDS', 'AVHHOLDSZ']), stl_counts_msoa, on="MSOA11CD")
f, ax = plt.subplots(figsize=(12,8))
ax = stl_merge_msoa.plot(ax=ax, column="counts", cmap="viridis", edgecolor="black", legend=True)
plt.title("Airbnb STL Listings Distribution in MSOAs")
plt.show()
```

```{python}
#| include: false
# kmeans for stl in london by borough
from sklearn.neighbors import NearestNeighbors
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, PowerTransformer
from sklearn.cluster import KMeans, DBSCAN 
from sklearn.metrics import silhouette_samples, silhouette_score

pivot_stl = stl_borough.groupby(['GSS_CODE','room_type'], observed=False)['index_right'].agg(Count='count').reset_index().pivot(index='GSS_CODE', columns='room_type', values='Count')         
pivot_stl.drop(['Hotel room', 'Shared room'], axis=1, inplace=True)
pivot_norm = pd.DataFrame(index=pivot_stl.index)
for cols in pivot_stl.columns:
    pivot_norm[cols]=PowerTransformer().fit_transform(pivot_stl[cols].to_numpy().reshape(-1, 1))
clu_median = stl_borough.groupby(by='GSS_CODE').price.agg('median')
pivot_norm["median price"] = pd.Series(np.squeeze(RobustScaler().fit_transform(clu_median.values.reshape(-1,1))), index=clu_median.index) 
#find suitable k
#k_range = []
#scores = []
#for k in range(2,32):
    #kmeans = KMeans(n_clusters=k, n_init=25, random_state=42).fit(pivot_norm)
    #ave_score = silhouette_score(pivot_norm, kmeans.labels_)
    #k_range.append(k)
    #scores.append(ave_score)
#plt.plot(k_range, scores)
#plt.gcf().suptitle("Average Silhouette Scores");
n = 5
kmeans = KMeans(n_clusters=n, n_init=25, random_state=42).fit(pivot_norm)
pivot_norm['cluster']=kmeans.labels_
result_clu_stl = boroughs.merge(pivot_norm, on="GSS_CODE")
f, ax = plt.subplots(figsize=(12,8))
result_clu_stl.plot(ax=ax, column="cluster", cmap="PuBu", edgecolor="black",legend=True)
plt.title("STL Cluster By London Boroughs")
plt.show()
col = ["Entire home/apt", "Private room", "median price"]
centroid_boro = pd.DataFrame(columns=col)
for k in sorted(result_clu_stl["cluster"].unique()):
    print(f"Processing cluster {k}")
    clust_boro = result_clu_stl[result_clu_stl["cluster"]==k]
    centroid_boro.loc[k] = clust_boro[col].mean()
```

```{python}
#| include: false
#density of hotels and attractions by borough
borough_density = gpd.read_file(data_cache('https://github.com/BiuLei0527/CASA0013_groupwork/raw/refs/heads/main/Borough_density.gpkg', dest))
borough_density = borough_density.to_crs(epsg=27700)
density = borough_density[['GSS_CODE','density_hotel', 'attractions_hotel']]
density.set_index('GSS_CODE', inplace=True)
density_st = StandardScaler().fit_transform(density) 
density_st_df = pd.DataFrame(density_st, columns=["density_hotel", "attractions_hotel"], index=density.index)
k_ran = []
score = []
for k in range(2,32):
    kmeans = KMeans(n_clusters=k, n_init=25, random_state=42).fit(density_st)
    average_score = silhouette_score(density_st, kmeans.labels_)
    k_ran.append(k)
    score.append(average_score)
plt.plot(k_ran, score)
plt.gcf().suptitle("Average Silhouette Scores");
i = 3
k_mean = KMeans(n_clusters=i, n_init=25, random_state=42).fit(density_st_df)
density_st_df['cluster']=k_mean.labels_
result_clu_density = boroughs.merge(density_st_df, on="GSS_CODE")
f, ax = plt.subplots(figsize=(12,8))
result_clu_density.plot(ax=ax, column="cluster", cmap='BuPu', edgecolor="black", legend=True)
plt.title("Hotels and Attractions By London Boroughs")
plt.show()
co = ["density_hotel", "attractions_hotel"]
centroid_dense = pd.DataFrame(columns=co)
for k in sorted(density_st_df["cluster"].unique()):
    print(f"Processing cluster {k}")
    clu_dense = density_st_df[density_st_df["cluster"]==k]
    centroid_dense.loc[k] = clu_dense[co].mean()
```

```{python}
#| include: false
#try k-means for msoa-level airbnb data
bnb_1 = bnb_gdf[bnb_gdf['minimum_nights']<=90][['price', 'room_type', 'property_type', 'minimum_nights', 'geometry', 'number_of_reviews']]
msoa_listings = gpd.sjoin(bnb_1, MSOA.drop(columns=['MSOA11NM', 'LAD11CD', 'LAD11NM', 'RGN11CD', 'RGN11NM','USUALRES', 'HHOLDRES', 'COMESTRES', 'POPDEN', 'HHOLDS', 'AVHHOLDSZ']), how="inner", predicate="within")
count_msoa = msoa_listings.groupby('MSOA11CD')['index_right'].agg('count').reset_index()
count_msoa['area_km2'] = count_msoa['MSOA11CD'].map(msoa_listings.groupby('MSOA11CD')['area_km2'].first())
count_msoa.rename(columns={'index_right': 'Count'}, inplace=True)
count_msoa.set_index('MSOA11CD', inplace=True)
count_msoa["density"]=round(count_msoa["Count"]/count_msoa["area_km2"],2)
count_msoa.drop(['Count', 'area_km2'], axis=1, inplace=True)
count_msoa['density'] = PowerTransformer().fit_transform(count_msoa['density'].to_numpy().reshape(-1, 1))
price_msoa = msoa_listings.groupby(by='MSOA11CD').price.agg('median')
count_msoa["median price"] = pd.Series(np.squeeze(RobustScaler().fit_transform(price_msoa.values.reshape(-1,1))), index=price_msoa.index) 
reviews_msoa = msoa_listings.groupby(by='MSOA11CD')["number_of_reviews"].agg('count')
count_msoa["mean reviews"] = pd.Series(np.squeeze(MinMaxScaler().fit_transform(reviews_msoa.values.reshape(-1,1))), index=reviews_msoa.index) 
#find suitable k
k_range = [] 
scores = []
for k in range(2,30):
    kmeans = KMeans(n_clusters=k, n_init=25, random_state=42).fit(count_msoa)
    ave_score = silhouette_score(count_msoa, kmeans.labels_)
    k_range.append(k)
    scores.append(ave_score)
plt.plot(k_range, scores)
plt.gcf().suptitle("Average Silhouette Scores");
#The Silhouette Scores is not good, so we choose DBSCAN
```

```{python}
#| include: false
#find knee point for dbscan then do dbscan
from kneed import KneeLocator
db_name = "DBSCAN"
nearest_nei = NearestNeighbors(n_neighbors=6).fit(count_msoa)
neigh_distance, _ = nearest_nei.kneighbors(count_msoa)  
neigh_distance = np.sort(neigh_distance, axis=0)
neigh_distance = neigh_distance[:, 1]
kn = KneeLocator(np.arange(neigh_distance.shape[0]), neigh_distance, S=12, curve='convex', direction='increasing')
kn.plot_knee()
kn.plot_knee_normalized()
epsilon_knee = neigh_distance[kn.knee]
print(f'the best epsilon is {epsilon_knee}')
dbscan = DBSCAN(eps=epsilon_knee, min_samples=count_msoa.shape[1]+1).fit(count_msoa.values)
clu_name = pd.Series(dbscan.labels_, index=count_msoa.index, name=db_name)
count_msoa[db_name] = clu_name
cols = ['density', 'median price', 'mean reviews']
centroid_mean = pd.DataFrame(columns=cols)
for k in sorted(count_msoa[db_name].unique()):
    print(f"Processing cluster {k}")
    cluster = count_msoa[count_msoa[db_name]==k]  
    centroid_mean.loc[k] = cluster[cols].mean()  
centroid_mean.drop(labels=[-1], axis=0, inplace=True) 
```

```{python}
#| include: false
msoa_clu = MSOA.merge(count_msoa, on="MSOA11CD")
f, ax = plt.subplots(figsize=(12,8))
msoa_clu.plot(ax=ax, column="DBSCAN", cmap="PuBu", edgecolor="black",legend=True)
plt.title("Listings Cluster on MSOAs")
plt.show()
```

```{python}
#| include: false
#read parquet, do one-hot encode
#msoa_atlas = pd.read_parquet("MSOA_Atlas.parquet")
msoa_atlas = pd.read_parquet(data_cache("https://github.com/BiuLei0527/CASA0013_groupwork/raw/refs/heads/main/MSOA_Atlas.parquet",dest))
from sklearn.preprocessing import OneHotEncoder
from umap import UMAP
msoa_atlas.set_index("MSOA Code", inplace=True)
boro_enco = OneHotEncoder(sparse_output=False)
sub_enco = OneHotEncoder(sparse_output=False)
encoded_borough = boro_enco.fit_transform(msoa_atlas[['Borough']])
encoded_subregion = sub_enco.fit_transform(msoa_atlas[['Subregion']])
encoded_boro = pd.DataFrame(encoded_borough, columns=boro_enco.get_feature_names_out(['Borough']),index=msoa_atlas.index)
encoded_sub = pd.DataFrame(encoded_subregion, columns=sub_enco.get_feature_names_out(['Subregion']),index=msoa_atlas.index)
```

```{python}
#| include: false
#choose cluster variables
cluster = msoa_atlas.drop(['MSOA Name', 'Borough', 'Subregion',
    "Age-All Ages", "Households-All Households", 
    "Economic Activity-Economically inactive: Total", 
    "Economic Activity-Economically active: Total", 
    "Total Median hh Income", "Detached", "Semi-detached",
    "Terraced (including end-terrace)", "Flat, maisonette or apartment", 
    "Population Density-Persons per hectare (2012)", 
    "Qualifications-Other", "Vehicles-Cars per hh", 
    "BAME", "Language-1+ English as a main language", 
    "Language-None have English as main language", 'Vehicles-Sum of all cars or vans in the area'], axis=1)
cluster = pd.concat([cluster, encoded_boro, encoded_sub], axis=1) 
```

```{python}
#| include: false
#do standardising
scaler_data = cluster.loc[:, 'Age-0-15':'Vehicles-4 or more cars or vans in hh'] 
scaler = RobustScaler().fit_transform(scaler_data)
cluster.loc[:, 'Age-0-15':'Vehicles-4 or more cars or vans in hh'] = scaler
```

```{python}
#| include: false
#reduce dimensionality using umap
dimensions=3 
umap = UMAP(n_neighbors=25, min_dist=0.05, n_components=dimensions, random_state=42)
umap_clu = umap.fit_transform(cluster)
print(umap_clu.shape)
clu_u = pd.DataFrame(index=cluster.index)
for i in range(0,umap_clu.shape[1]): 
    print(i)
    u = pd.Series(umap_clu[:,i], index=cluster.index) 
    clu_u[f"Dimension {i+1}"] = u
```

```{python}
#| include: false
clu_u.head()
```

```{python}
#| include: false
#dbscan for MSOA-level community based on above socio-economic characteristics
db_name1 = "DBSCAN"
nearest_neighbor = NearestNeighbors(n_neighbors=6).fit(clu_u)
neigh_distance1, _ = nearest_neighbor.kneighbors(clu_u)  
neigh_distance1 = np.sort(neigh_distance1, axis=0)
neigh_distance1 = neigh_distance1[:, 1]
kn1 = KneeLocator(np.arange(neigh_distance1.shape[0]), neigh_distance1, S=12, curve='convex', direction='increasing')
kn1.plot_knee()
kn1.plot_knee_normalized()
epsilon_knee1 = neigh_distance1[kn1.knee]
print(f'the best epsilon is {epsilon_knee1}')
dbscan1 = DBSCAN(eps=epsilon_knee1, min_samples=clu_u.shape[1]+1).fit(clu_u.values)
clu_name1 = pd.Series(dbscan1.labels_, index=clu_u.index, name=db_name1)
clu_u[db_name1] = clu_name1
cols1 = ['Dimension 1', 'Dimension 2', 'Dimension 3']
centroid_mean1 = pd.DataFrame(columns=cols1)
for k in sorted(clu_u[db_name1].unique()):
    print(f"Processing cluster {k}")
    cluster1 = clu_u[clu_u[db_name1]==k]  
    centroid_mean1.loc[k] = cluster1[cols1].mean()  
centroid_mean1.drop(labels=[-1], axis=0, inplace=True) 
```

```{python}
#| include: false
centroid_mean1.to_csv('centroid_mean.csv', index=True)
```

```{python}
#| include: false
msoa_social = MSOA.merge(clu_u, left_on="MSOA11CD", right_index=True)
f, ax = plt.subplots(figsize=(12,8))
msoa_social.plot(ax=ax, column="DBSCAN", cmap="PuBu", edgecolor="black",legend=True)
plt.title("Community clustering at the msoa level")
plt.show()
```

```{python}
#| include: false
#try k-means for msoa-level airbnb data
bnb_1 = bnb_gdf[bnb_gdf['minimum_nights']<=90][['price', 'room_type', 'property_type', 'minimum_nights', 'geometry', 'number_of_reviews']]
msoa_listings = gpd.sjoin(bnb_1, MSOA.drop(columns=['MSOA11NM', 'LAD11CD', 'LAD11NM', 'RGN11CD', 'RGN11NM','USUALRES', 'HHOLDRES', 'COMESTRES', 'POPDEN', 'HHOLDS', 'AVHHOLDSZ']), how="inner", predicate="within")
count_msoa = msoa_listings.groupby('MSOA11CD')['index_right'].agg('count').reset_index()
count_msoa['area_km2'] = count_msoa['MSOA11CD'].map(msoa_listings.groupby('MSOA11CD')['area_km2'].first())
count_msoa.rename(columns={'index_right': 'Count'}, inplace=True)
count_msoa.set_index('MSOA11CD', inplace=True)
count_msoa["density"]=round(count_msoa["Count"]/count_msoa["area_km2"],2)
count_msoa.drop(['Count', 'area_km2'], axis=1, inplace=True)
count_msoa['density'] = PowerTransformer().fit_transform(count_msoa['density'].to_numpy().reshape(-1, 1))
price_msoa = msoa_listings.groupby(by='MSOA11CD').price.agg('median')
count_msoa["median price"] = pd.Series(np.squeeze(RobustScaler().fit_transform(price_msoa.values.reshape(-1,1))), index=price_msoa.index) 
reviews_msoa = msoa_listings.groupby(by='MSOA11CD')["number_of_reviews"].agg('count')
count_msoa["mean reviews"] = pd.Series(np.squeeze(MinMaxScaler().fit_transform(reviews_msoa.values.reshape(-1,1))), index=reviews_msoa.index) 

#find suitable k
k_range = [] 
scores = []
for k in range(2,30):
    kmeans = KMeans(n_clusters=k, n_init=25, random_state=42).fit(count_msoa)
    ave_score = silhouette_score(count_msoa, kmeans.labels_)
    k_range.append(k)
    scores.append(ave_score)
#plt.plot(k_range, scores)
#plt.gcf().suptitle("Average Silhouette Scores");

#The Silhouette Scores is not good, so we choose DBSCAN
#find knee point for dbscan then do dbscan
from kneed import KneeLocator
db_name = "DBSCAN"
nearest_nei = NearestNeighbors(n_neighbors=6).fit(count_msoa)
neigh_distance, _ = nearest_nei.kneighbors(count_msoa)  
neigh_distance = np.sort(neigh_distance, axis=0)
neigh_distance = neigh_distance[:, 1]
kn = KneeLocator(np.arange(neigh_distance.shape[0]), neigh_distance, S=12, curve='convex', direction='increasing')
#kn.plot_knee()
#kn.plot_knee_normalized()
epsilon_knee = neigh_distance[kn.knee]
print(f'the best epsilon is {epsilon_knee}')
dbscan = DBSCAN(eps=epsilon_knee, min_samples=count_msoa.shape[1]+1).fit(count_msoa.values)
clu_name = pd.Series(dbscan.labels_, index=count_msoa.index, name=db_name)
count_msoa[db_name] = clu_name
cols = ['density', 'median price', 'mean reviews']
centroid_mean = pd.DataFrame(columns=cols)
for k in sorted(count_msoa[db_name].unique()):
    print(f"Processing cluster {k}")
    cluster = count_msoa[count_msoa[db_name]==k]  
    centroid_mean.loc[k] = cluster[cols].mean()  
centroid_mean.drop(labels=[-1], axis=0, inplace=True) 
msoa_clu = MSOA.merge(count_msoa, on="MSOA11CD")
f, ax = plt.subplots(figsize=(12,8))
msoa_clu.plot(ax=ax, column="DBSCAN", cmap="PuBu", edgecolor="black")
plt.title("STL Cluster on MSOAs")
plt.show()
```

## 1. Who collected the InsideAirbnb data?

Murray Cox founded InsideAirbnb in 2015 and began collecting Airbnb data [@PuckLo]. The website now has collaborators, new members, and an advisory board to support project development and data collection. 

## 2. Why did they collect the InsideAirbnb data?

They collected the InsideAirbnb data to address social concerns about Airbnb's impact on affordable housing and gentrification [@Katz].  In 2015, they discovered that Airbnb data could be misleading, hiding the true scale of its effects on communities. The purpose of collecting Inside Airbnb data is to provide transparency, advocate for community housing, and protect the rights of tenants and small landlords. 

## 3. How was the InsideAirbnb data collected?  

The data is collected through python scripts, which are sourced from Github and other online resource [@Alsudais2021], each listing page on the website.

The data collection can be divided into the following stages: Firstly, finding all public listings on the Airbnb website as much as possible. Aditionally, accessing each page through the script to collect information such as the ID of the listing, the type of home, the time it was published, the number of reviews, and the location. Finally, aggregating the data and verifying it with the number of data published by Airbnb. They use the number of reviews as the number of visits to the listing for they lack internal data [@CoxSlee].

They state that no private information is used in the data collection process and that the names of the listings are compiled by comparing the geographical coordinates of the listings with the city-to-community definitions. Furthermore, in the later development, 50% of the comment rate was converted to an estimated booking and an average length of stay was assigned to each city [@Insideairbnb_data_assumptions].

## 4. How does the method of collection impact the completeness and/or accuracy of the InsideAirbnb data set's representation of the process it seeks to study, and what wider issues does this raise?

It is obvious that the main data source of IA (Inside Airbnb) lacks accuracy, as it is obtained by directly scraping data collected by Airbnb itself from its website [@CoxSlee]. In addition, other data sources for the IA dataset are also unreliable, which are related to the data collection methods of Python scripts. Specifically, Python scripts are likely to generate IA datasets by copying scripts that lack responsible maintenance on any websites [@Insideairbnb_behind], with GitHub being an example [@Alsudais2021]. Moreover, the automated generation of code for data collection services for IA datasets may encounter issues such as ignoring list types and making incorrect links [@Alsudais2021], which can lead to problems include duplicating data collection, data position errors, and data omissions. For example, automated code may simplify or truncate some property review fields.

Overall, the IA dataset covers a wide range of data, such as various housing types and different landlord characteristics. This is beneficial for the process of data research. However, unreliable data sources and automatically generated codes affect the research results of IA. In addition, due to the setting of monthly updates [@Alsudais2021], the dataset of IA is not updated in real time, which brings the problem that the existing dataset cannot represent the real data. As a result, the IA dataset can only representbthe process it wishes to study to some extent. Specifically, only when the research scope, problem definition, time constraints, and data processing methods are clearly defined, can the IA dataset effectively reflect analytical research.

These limitations have resulted in serious consequences. For reseachers who use inaccurate IA datasets for research, the reproducibility of their research results will be doubted. Additionally, as the errors in the IA dataset increase, the public's trust in online information will decrease accordingly. Furthermore, as an important reference, if the IA dataset lacks accuracy, relevant political and economic decisions may be misled.

## 5. What ethical considerations does the use of the InsideAirbnb data raise? 

We analyze its ethical issues through four stages: purpose of data use，data collection, data storage, use and impact of analysis results. 

### 5.1 Purpose of Data Use
- The transparency of InsideAirbnb itself: InsideAirbnb's mission statement is to protect the city from short-term rentals [@Insideairbnb_about]. InsideAirbnb has a bias when analyzing data, which may reduce the objectivity of the results and lead to inaccuracy.

### 5.2 Data Collection
- Data source: InsideAirbnb says it’s not endorsed by Airbnb [@Insideairbnb_data_assumptions]. Airbnb expressly states that it prohibits using automated means to access or collect data from the Airbnb Platform. But whether InsideAirbnb collects its data through Python scripts may involve legal and ethical controversies that should be considered. 
- Data privacy and security: Being able to access or collect data does not mean that it is ethical to use that data [@Boyd2014networked]. Although InsideAirbnb claims in its disclaimer that the data is safe and full-protected, it is risky that it involves sensitive content such as names, photos, locations and reviews, which may indirectly expose individuals' privacy, and trigger ethical judging of data collection and privacy security. 

### 5.3 Data storage 
- Fairness of data access: InsideAirbnb provides the most recent 12 months of data for free, but access to archived data is subject to review or even payment [@Insideairbnb_data_assumptions], which may be a hindrance to researchers or organizations with limited resources. 

### 5.4 Usage and Impacts of Analysis Results
- A risk of misuse: The analysis results derived from public data may be influenced by the researcher's position to support unfair policies or biased conclusions. 
- Unfair business competition: conclusions and analyses based on InsideAirbnb data could be exploited by competitors in the same industry to harm Airbnb or landlords engaged in lawful operations. 
- Challenges of public distrust: deepening public distrust and doubt about the sharing economy, inducing people to overlook the economic benefits that platforms like Airbnb may bring to certain landlords and tenants. 

## 6. With reference to the InsideAirbnb data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London? 

### 6.1 Spatial agglomeration 

```{python}
# Distribution of listings by boroughs and its median price
bnb_borough=gpd.sjoin(bnb_gdf, boroughs, how="inner", predicate="within")
bnb_borough.drop(index=bnb_borough[bnb_borough.NAME.isna()].index, axis=1, inplace=True)
bnb_counts_price=bnb_borough.groupby("NAME").agg(count=('NAME', 'count'), median=('price', 'median')).reset_index()
bnb_merge=pd.merge(boroughs, bnb_counts_price, on="NAME")
f, ax=plt.subplots(1,2,gridspec_kw={'width_ratios':[6,6]}, figsize=(12,8))
bnb_merge.plot(ax=ax[0],column='median',legend=True,cmap='plasma')
ax[0].set_title("Median Price", size=18)
bnb_merge.plot(ax=ax[1],column='count',legend=True,cmap='viridis')
ax[1].set_title("Count", size=18)
for x in ax:
    x.set_xticks([]) 
    x.set_yticks([])
    x.set_facecolor((0.8, 0.9, 1, 1))
    x.spines['left'].set_visible(False)
    x.spines['right'].set_visible(False)
    x.spines['top'].set_visible(False)
    x.spines['bottom'].set_visible(False)
    x.grid(which='major', visible=True, axis='both', color='lightgrey', linestyle='-', linewidth=0.5, zorder=0)
    x.set_axisbelow(True)
f.suptitle('Figure 1: Distribution of Listings and its Price by Boroughs', x=0.025, ha='left', size=22)
plt.show()
```

As shown in the figure 1, the distribution of the price and count of Airbnb listings in London shows a significant spatial clustering, with the highest values in the city center (along the River Thames), while lower values in the outskirts of the city, showing a decreasing spatial distribution pattern from the center. 

### 6.2 Market-Oriented

Landlord market-oriented: The influence of professional hosts is significant. Among all kinds of landlords, although 81% of single-unit landlords have an absolute advantage in terms of quantity, they only control 49% of the units. By comparison, only 19% of professional hosts, who are landlords with multiple properties [@Li2016pros], control 51% of properties. 

```{python}
listings_type = df.loc[:, ['room_type', 'latitude', 'longitude', 'number_of_reviews', 'price']]
listings = listings_type.groupby('room_type').agg(
    count=('room_type', 'count'),
    price=('price', 'median')
).reset_index()

room_types = listings['room_type']
room_counts = listings['count']
median_prices = listings['price']

listings['proportion'] = listings['count'] / listings['count'].sum()

fig, ax1 = plt.subplots(figsize=(10, 6))
bars = ax1.bar(room_types, room_counts, color='lightblue', label='Number of Listings')
for bar in bars:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width() / 2, height, f'{int(height)}', ha='center', va='bottom')

ax2 = ax1.twinx()
ax2.plot(room_types, median_prices, color='green', marker='s', linestyle='--', label='Median Price')
ax1.set_xlabel("Room Type")
ax1.set_ylabel("Number of Listings", color='blue')
ax2.set_ylabel("Median Price ($)", color='green')
plt.title("Chart 1: Median Price by Room Type in London")
fig.legend(loc='upper left', bbox_to_anchor=(0.7, 0.85))
plt.tight_layout()
plt.show()
```

Property market-oriented: chart 1 shows that the proportion of properties of essential rental property types is the highest, accounting for 63.32%. When the housing is rented out in the form of rooms, its main function is still residential, which does not affect the supply of the housing rental market [@Chang2020disrupt]. However, as a large number of landlords place their entire properties in the Short-term Lets market, their properties shift towards commercial attributes. 

### 6.3 Primarily for Short-Term Lets 

According to the Deregulation Act 2015, Short-term Lets (STL) is defined as "The total number of nights of use does not exceed 90 nights in a calendar year" [@UKparliament2015]. Based on InsideAirbnb data, 97.3% of listings in London are classified as STL, indicating that the Airbnb market is largely made up of STL. Therefore, the analysis focuses on the spatial distribution characteristics and types of STL [@Insideairbnb2024]. 

### 6.4 Summary 

The Airbnb rental market in London has clear characteristics of spatial agglomeration, market-oriented, and short-term leasing. This market can meet short-term accommodation needs. However, it has potential negative impacts on the long-term rental market. The government needs to introduce policies to strengthen the regulation of the STL market. 

## 7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, and statistical analysis/models), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London? 

Based on the clustering analysis results of the InsideAirbnb dataset, this section explores the characteristics of STL markets in different boroughs of London clusters. Then, several regulatory recommendations were put forward to promote the sustainable and multi-center development of the STL market. 

```{python}
# kmeans for stl in london by borough
from sklearn.neighbors import NearestNeighbors
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, PowerTransformer
from sklearn.cluster import KMeans, DBSCAN 
from sklearn.metrics import silhouette_samples, silhouette_score

bnb_stl = bnb_gdf[bnb_gdf['minimum_nights']<=90][['price', 'room_type', 'property_type', 'minimum_nights', 'geometry','number_of_reviews','availability_365']]
stl_borough = gpd.sjoin(bnb_stl, boroughs, how="inner", predicate="within")
pivot_stl = stl_borough.groupby(['GSS_CODE','room_type'], observed=False)['index_right'].agg(Count='count').reset_index().pivot(index='GSS_CODE', columns='room_type', values='Count')         
pivot_stl.drop(['Hotel room', 'Shared room'], axis=1, inplace=True)
pivot_norm = pd.DataFrame(index=pivot_stl.index)
for cols in pivot_stl.columns:
    pivot_norm[cols]=PowerTransformer().fit_transform(pivot_stl[cols].to_numpy().reshape(-1, 1))
clu_median = stl_borough.groupby(by='GSS_CODE').price.agg('median')
pivot_norm["median price"] = pd.Series(np.squeeze(RobustScaler().fit_transform(clu_median.values.reshape(-1,1))), index=clu_median.index) 
reviews_mean = stl_borough.groupby(by='GSS_CODE')["number_of_reviews"].agg('mean')
pivot_norm["mean reviews"] = pd.Series(np.squeeze(MinMaxScaler().fit_transform(reviews_mean.values.reshape(-1,1))), index=reviews_mean.index)

#find suitable k
k_range = []
scores = []
for k in range(2,32):
    kmeans = KMeans(n_clusters=k, n_init=25, random_state=42).fit(pivot_norm)
    ave_score = silhouette_score(pivot_norm, kmeans.labels_)
    k_range.append(k)
    scores.append(ave_score)
#plt.plot(k_range, scores)
#plt.gcf().suptitle("Average Silhouette Scores");
n = 5
kmeans = KMeans(n_clusters=n, n_init=25, random_state=42).fit(pivot_norm)
pivot_norm['cluster']=kmeans.labels_
result_clu_stl = boroughs.merge(pivot_norm, on="GSS_CODE")
f, ax = plt.subplots(figsize=(12,8))
result_clu_stl.plot(ax=ax, column="cluster", cmap="Blues", edgecolor="black",legend=True)
plt.title("Figure 2: STL Cluster By London Boroughs")
plt.show()
col = ["Entire home/apt", "Private room", "median price","mean reviews"]
centroid_boro = pd.DataFrame(columns=col)
for k in sorted(result_clu_stl["cluster"].unique()):
    #print(f"Processing cluster {k}")
    clust_boro = result_clu_stl[result_clu_stl["cluster"]==k]
    centroid_boro.loc[k] = clust_boro[col].mean()
```

Using the standardized variables "Entire home/apt," "Private room," and "median price," London is divided into 5 clusters (Figure 2). The central areas (Cluster 4, Cluster 2, and Cluster 0) are high-density short-term rental areas, dominated by entire home listings with high prices. The outer areas (Cluster 1 and Cluster 3) are low-density short-term rental areas with limited housing supply. According to these characteristics, the following regulatory suggestions can be made. 

### 7.1 Price Regulation 

High-density short-term rental areas (central areas): strictly enforce the 90-day rental limit to prevent housing resources from being taken away from long-term lets. Apply tiered taxes based on rental prices to stabilize housing costs. 

Low-density short-term rental areas (outer areas): introduce incentives to reduce housing pressure in the central areas. For example, offer tax benefits or subsidies to attract more landlords to the outer areas and create a multi-center short-term lets market in London. 

### 7.2 Area-Based Regulation 

Restricting entire rentals in high-density areas: As there is a squeeze on the long-term housing market due to STL of entire properties (chart 2), stricter policies on STL of entire properties should be implemented in areas with dense housing stock and high rental pressures (e.g. central London), such as limiting the number of days or properties of STL. 

Considering the distribution of regional STL properties, implement differentiated regulatory policies and impose different day limits for areas with different densities. 

```{python}
stl_rtype = stl['room_type'].value_counts()
stl_rtype.head()
stl_ptype = stl['property_type'].value_counts().reset_index()
stl_ptype_7 = stl_ptype[:7]
other_types = pd.DataFrame({"property_type":['Others'], "count":[stl_ptype['count'][7:].sum()]})
stl_ptype2 = pd.concat([stl_ptype_7, other_types], ignore_index=True)
pastel_cmap = plt.get_cmap('Pastel1', len(stl_ptype2['count']))
colors = [pastel_cmap(i) for i in range(len(stl_ptype2['count']))]
plt.figure(figsize=(8, 6))
plt.pie(stl_ptype2['count'], 
        labels=stl_ptype2['property_type'], 
        autopct='%1.1f%%', 
        startangle=90, 
        colors=colors) 
plt.title("Chart 2: STL Property Type Pie Chart")
plt.show()
```

### 7.3 Special supervision for professional hosts 

Figure 3 shows that a small number of professional hosts have a large number of STL properties and extremely high market share, which may be related to their commercial operations. In addition, professional hosts have lower management marginal costs because of operating multiple properties simultaneously [@Xie2021professional], which will give them a more advantageous competitive position in the market, ultimately leading to unfair competition. 

In response to this phenomenon, it is recommended to implement special supervision for professional hosts: 

```{python}
# Filter hosts with more than 100 short-term rental listings
short_term_rentals = df[(df["minimum_nights"] <= 90) & (df["availability_90"] > 0)]
host = short_term_rentals[["host_name", "host_id", "calculated_host_listings_count"]]
host_counts = host.groupby("host_id")["calculated_host_listings_count"].max().reset_index()
host_counts = host_counts.sort_values(by="calculated_host_listings_count", ascending=False)
hosts_over_100 = host_counts[host_counts["calculated_host_listings_count"] >= 100]
hosts_filtered = pd.merge(hosts_over_100, short_term_rentals[["host_id", "host_name"]], on="host_id").drop_duplicates()
#print(f"There are {hosts_filtered.shape[0]} hosts with more than 100 short-term listings in the dataset.")
f, ax = plt.subplots(figsize=(8, 6))
hosts_filtered.plot(ax=ax, kind="bar", x="host_name", y="calculated_host_listings_count", color="SkyBlue")
ax.set_title("Figure 3: Hosts With More Than 100 STL")
ax.set_xlabel("Host Name")
ax.set_ylabel("Number of Short-Term Listings")
plt.tight_layout()
plt.show()
```

- Limit on the Number of Listings: Set limitations on the number of STL a host can operate.  Implement stricter regulations on hosts with multiple properties to prevent professional hosts from dominating the market. 
- Implementation of Permitting Mechanism: Require hosts with multiple properties to apply for a commercial permit to ensure legality and compliance. 
- Additional Tax Policies: Implement a progressive tax system based on the number of STL properties owned by a host, where the more properties a host operates, the higher the applicable tax rate. Additionally, impose urban development tax and housing security surcharge on professional hosts, using the revenue to fund Long-Term Lets supply and community welfare programs. 

## References
