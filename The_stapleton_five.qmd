---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Group Name's Group Project
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto Flex
    monofont: InputMonoCondensed
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python (base)
    language: python
    name: base
---

```{python}
%matplotlib inline
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt  
import seaborn as sns
import geopandas as gpd
import requests
from requests import get
from urllib.parse import urlparse
from functools import wraps
import matplotlib
```

```{python}
#Refer to Practical 10 "https://jreades.github.io/fsds/practicals/Practical-10-Presenting_Data.html"
def cache_che(func):
    @wraps(func)
    def wrapper(url_p, dest):
        url=urlparse(url_p)
        file_name=os.path.split(url.path)[-1]
        dest_name=os.path.join(dest, file_name)
        if os.path.isfile(dest_name):
            print(f'{dest_name} have been found locally')
            return(dest_name)
        else:
            print(f'{dest_name} isn\'t here, it is downlowded now')
            return(func(url_p, dest_name))
    return wrapper
@cache_che
def data_cache(url_p:str, dest:str) -> str:
    path = os.path.split(dest)[0]
    if path != '':
        os.makedirs(path, exist_ok=True)
    with open(dest, "wb") as file: 
        response = get(url_p)
        file.write(response.content) 
    return dest
```

```{python}
host = 'https://orca.casa.ucl.ac.uk'
path = '~jreades/data'
file = '20240614-London-listings.csv.gz'
url  = f'{host}/{path}/{file}'

if os.path.exists(file):
  df = pd.read_csv(file, compression='gzip', low_memory=False)
else: 
  df = pd.read_csv(url, compression='gzip', low_memory=False)
  df.to_csv(file, compression='gzip', index=False)
```

## 1. Who collected the InsideAirbnb data?

::: {.10/11/2024}

( 2 points; Answer due Week 7 )

::: 

Inside Airbnb, founded in 2015, its founder Murray Cox began conceiving the project in 2014 and started collecting Airbnb data and aggregating it into an interactive web-based interface [@PuckLo]. After years of development, the website has collaborators, new members, and advisory board to help with project development, data collection etc.

## 2. Why did they collect the InsideAirbnb data?

::: {.11/11/2024}

( 4 points; Answer due Week 7 )

:::

This project was inspired by the DIVAS summer camp project in 2014. Afterwards, Murray considered the reasons for displacement and began to reflect on ‘how Airbnb is used in my community.' [@PuckLo, @insideairbnb_about].

More important reasons come from the social concerns and issues raised by Airbnb. Murray, criticising the sharing economy, argues that it ‘threatens affordable housing and exacerbates gentrification' [@Katz], where individuals transfer underutilised resources to others, such as Uber and Airbnb [@alsudais]. In 2015, Murray and Tom Slee discovered that many listings had disappeared from Airbnb data of New York, which was a misleading depiction of their business and went against Airbnb's original vision and commitment to the community.

It is based on these inspirations and facts that Inside Airbnb has been working hard to collect data about Airbnb worldwide and stand up for the development of community housing and the rights of tenants and single room landlords.

```{python}
print(f"Data frame is {df.shape[0]:,} x {df.shape[1]:,}")
```

```{python}
unique_id = df['host_id'].dropna().unique()
df1 = df[df["host_id"].isin(unique_id)]
```

## 3. How was the InsideAirbnb data collected?  

::: {.12/11/2024}

( 5 points; Answer due Week 8 )

:::

Inside Airbnb provides large open dataset that collects data from the official public listings on the Airbnb website, including detailed descriptions, property types, and user reviews. The data is collected through python scripts, which are sourced from Github and other online resource [@alsudais], each listing page on the website.

The data collection can be divided into the following stages: First, finding all public listings on the Airbnb website as much as possible. Second, accessing each page through the script to collect information such as the ID of the listing, the type of home, the time it was published, the number of reviews, and the location. Finally, aggregating the data and verifying it with the number of data published by Airbnb. They use the number of reviews as the number of visits to the listing for they lack internal data [@CoxSlee].

They state that no private information is used in the data collection process and that the names of the listings are compiled by comparing the geographical coordinates of the listings with the city-to-community definitions. Also in a later development, 50% of the comment rate was converted to an estimated booking and an average length of stay was assigned to each city [@insideairbnb_data_assumptions].

## 4. How does the method of collection impact the completeness and/or accuracy of the InsideAirbnb data set's representation of the process it seeks to study, and what wider issues does this raise?

::: {.15/11/2024}

( 11 points; Answer due Week 9 )

:::

It is obvious that the main data source of IA (Inside Airbnb) lacks accuracy, as it is obtained by directly scraping data collected by Airbnb itself from its website [@CoxSlee]. In addition, other data sources for the IA dataset are also unreliable, which are related to the data collection methods of Python scripts. Specifically, Python scripts are likely to generate IA datasets by copying scripts that lack responsible maintenance on any websites [@insideairbnb_behind], with GitHub being an example [@alsudais]. Moreover, the automated generation of code for data collection services for IA datasets may encounter issues such as ignoring list types and making incorrect links [@alsudais], which can lead to problems include duplicating data collection, data position errors, and data omissions. For example, automated code may simplify or truncate some property review fields.

Overall, the IA dataset covers a wide range of data, such as various housing types and different landlord characteristics. This is beneficial for the process of data research. However, unreliable data sources and automatically generated codes affect the research results of IA. In addition, due to the setting of monthly updates [@alsudais], the dataset of IA is not updated in real time, which brings the problem that the existing dataset cannot represent the real data. As a result, the IA dataset can only representbthe process it wishes to study to some extent. Specifically, only when the research scope, problem definition, time constraints, and data processing methods are clearly defined, can the IA dataset effectively reflect analytical research.

The formation of IA datasets due to inaccurate collection methods has resulted in more serious consequences, including impacts on research results, public trust towards the internet, and political and financial advice. For reseachers who use inaccurate IA datasets for research, the reproducibility of their research results will be doubted. Additionally, as the errors in the IA dataset increase, the public's trust in online information will decrease accordingly. Furthermore, as an important reference, if the IA dataset lacks accuracy, relevant political and economic decisions may be misled.

## 5. What ethical considerations does the use of the InsideAirbnb data raise? 

::: {.18/11/2024}

( 18 points; Answer due {{< var assess.group-date >}} )

:::

We analyze its ethical issues through four stages: purpose of data use，data collection, data storage, use and impact of analysis results.

1. Purpose of Data Use:
The transparency of Inside Airbnb itself: Inside Airbnb's mission statement is to protect the city from short-term rentals [@insideairbnb_about]. When using Inside Airbnb data for research, there is a need to consider the alignment of the research objectives with its mission. This may diminish the objectivity of the data and lead to biased results in the analysis.

2. Data Collection:
Data source: Inside Airbnb says it’s not endorsed by Airbnb [@insideairbnb_data_assumptions]. Airbnb expressly states that it prohibits using automated means to access or collect data from the Airbnb Platform. But whether Inside Airbnb collects its data through Python scripts may involve legal and ethical controversies that should be considered.

The accuracy of Inside Airbnb data: Inside Airbnb says it is not responsible for the accuracy of the information [@insideairbnb_data_assumptions]. It just provides snapshot data at a point in time and anonymizes listing location information. These can affect the timeliness and accuracy of the analysis.

Data privacy and security: Being able to access or collect data does not mean that it is ethical to use that data [@boyd2014networked]. Although Inside Airbnb claims in its disclaimer that the data is safe and full-protected, it is risky that it involves sensitive content such as names, photos, locations and reviews, which may indirectly expose individuals' privacy, and trigger ethical judging of data collection and privacy security.

3. Data storage
Fairness of data access: Inside Airbnb provides the most recent 12 months of data for free, but access to archived data is subject to review or even payment [@insideairbnb_data_assumptions], which may be a hindrance to researchers or organizations with limited resources.

Security Risks: Long-term storage of archived data may be subject to security risks associated with data aging, storage media damage, or technology updates. Without regular security audits and backup management, data security may be jeopardized.

4. Use and Impacts of Analysis Results:
A risk of misuse: The analysis results derived from public data could be taken out of context to support unfair policies or biased conclusions.
Unfair business competition: conclusions and analyses based on Inside Airbnb data could be exploited by competitors in the same industry to harm Airbnb or landlords engaged in lawful operations.

Challenges of public distrust: deepening public distrust and doubt about the sharing economy, inducing people to overlook the economic benefits that platforms like Airbnb may bring to certain landlords and tenants.

## 6. With reference to the InsideAirbnb data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London? 

::: {.duedate}

( 15 points; Answer due {{< var assess.group-date >}} )

:::

## 7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, and statistical analysis/models), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London? 

::: {.duedate}

( 45 points; Answer due {{< var assess.group-date >}} )

:::

## Sustainable Authorship Tools

Your QMD file should automatically download your BibTeX file. We will then re-run the QMD file to generate the output successfully.

Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) (<span style="font-family:Sans-Serif;">sansfont</span>) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`). 

## References

```{python}
#replace $ with blankspace
df['price']=df['price'].str.replace('$','',regex=False).str.replace(',', '', regex=False).astype(float) #convert price into float
```

```{python}
#use request-url to read boro
url = 'https://github.com/jreades/i2p/blob/master/data/src/Boroughs.gpkg?raw=true'
file_name='Boroughs.gpkg'
with open(file_name, "wb") as file: 
    response = get(url)
    file.write(response.content) 
boroughs = gpd.read_file(file_name)
```

```{python}
#use function to read boros 
path = 'https://github.com/jreades/fsds/blob/master/data/src/' 
dest = 'Data'
boroughs = gpd.read_file(data_cache(path+'Boroughs.gpkg?raw=true', dest))
gdf_bnb = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude, crs='epsg:4326'))
bnb_gdf = gdf_bnb.to_crs(boroughs.crs)
```

```{python}
#plot distribution of different room types in London
import plotly.graph_objects as go
boroughs_json=boroughs.__geo_interface__
fig = go.Figure()
fig.add_trace(go.Choroplethmapbox(geojson=boroughs_json, locations=boroughs['NAME'], colorscale="Cividis", marker=dict(opacity=0.75), showscale=False))
types=bnb_gdf['room_type'].unique().tolist()
colors=['#A23B72','#6CB4EE','#3D315B','#F2C641'] 
for t in types:
    subset=bnb_gdf[bnb_gdf['room_type']==t]
    fig.add_trace(go.Scattermapbox(lon=subset['longitude'],lat=subset['latitude'], mode='markers', marker=dict(size=5, color=colors[types.index(t)]), name=t))
fig.update_layout(title="Airbnb Listings by Room Type", legend=dict(title="Room Type"), mapbox=dict(style="open-street-map", center=dict(lat=bnb_gdf['latitude'].mean(), lon=bnb_gdf['longitude'].mean()),zoom=15))
fig.show()
```

```{python}
# Distribution of listings by boroughs and its median price
bnb_borough=gpd.sjoin(bnb_gdf, boroughs, how="inner", predicate="within")
bnb_borough.drop(index=bnb_borough[bnb_borough.NAME.isna()].index, axis=1, inplace=True)
bnb_counts_price=bnb_borough.groupby("NAME").agg(count=('NAME', 'count'), median=('price', 'median')).reset_index()
bnb_merge=pd.merge(boroughs, bnb_counts_price, on="NAME")
f, ax=plt.subplots(1,2,gridspec_kw={'width_ratios':[6,6]}, figsize=(12,8))
bnb_merge.plot(ax=ax[0],column='median',legend=True,cmap='plasma')
ax[0].set_title("Median Price", size=18)
bnb_merge.plot(ax=ax[1],column='count',legend=True,cmap='viridis')
ax[1].set_title("Count", size=18)
for x in ax:
    x.set_xticks([]) 
    x.set_yticks([])
    x.set_facecolor((0.8, 0.9, 1, 1))
    x.spines['left'].set_visible(False)
    x.spines['right'].set_visible(False)
    x.spines['top'].set_visible(False)
    x.spines['bottom'].set_visible(False)
    x.grid(which='major', visible=True, axis='both', color='lightgrey', linestyle='-', linewidth=0.5, zorder=0)
    x.set_axisbelow(True)
f.suptitle('Distribution of Listings and its Price by Boroughs', x=0.025, ha='left', size=22)
plt.figtext(x=0.025, y=0.88, s=f"Total listings: {bnb_merge['count'].sum()}", size=14)
```

```{python}
#room type with counts, reviews and price
from bokeh.plotting import figure 
from bokeh.io import output_notebook, show 
from bokeh.models import ColumnDataSource, HoverTool 
from bokeh.palettes import Spectral4 
output_notebook()
listings_type = df.loc[:, ['room_type','latitude', 'longitude','number_of_reviews','price']]
listings = listings_type.groupby('room_type').agg(count=('room_type', 'count'), mean=('number_of_reviews', 'mean'), price=('price', 'median')).reset_index()
room_types = listings['room_type'].to_list()
reviews = listings['mean'].to_list()
room_counts = listings['count'].to_list()
prices = listings['price'].to_list()
source = ColumnDataSource(data=dict(types=room_types, counts=room_counts, reviews=reviews, price=prices))
tool_tips = [("Room Type", "@types"), ("Number of Listings", "@counts{,}"),("Mean of Reviews", "@reviews{,}"), ("Median Price", "$@price{,}/night")]# ‘,’ represent thousandth
p1 = figure(x_range=room_types, height=300, tooltips=tool_tips, title=f"Mean Reviews and Median Price by Room Type in London", toolbar_location=None, tools="") 
p1.vbar(x='types', top='counts', width=0.9, source=source) 
#p.xgrid.grid_line_color = None
p1.y_range.start = 0
p1.xaxis.axis_label = "Room Type"
p1.yaxis.axis_label = "Number of Listings"
p1.title.text_font_size = '18pt'
show(p1)
```

```{python}
listings_type = df.loc[:, ['room_type','latitude', 'longitude','number_of_reviews']]
counts = listings_type['room_type'].value_counts().to_dict()
orders = ['Entire home/apt', 'Private room', 'Shared room', 'Hotel room']
plt.figure(figsize=(8,6))
ax=sns.countplot(data=listings_type, x='room_type', palette="dark", hue=listings_type['room_type'])
for c in ax.containers:
    labels = [counts[l] for l in counts.keys()]
    ax.bar_label(c, label=labels, fontsize=10)
plt.title('Counts of Different Room Type in London')
plt.show()
```

```{python}
#word cloud
import nltk
import re
from nltk.tokenize import word_tokenize, sent_tokenize 
from nltk.tokenize.toktok import ToktokTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.feature_extraction.text import CountVectorizer 
from sklearn.decomposition import LatentDirichletAllocation
from nltk import ngrams, FreqDist
from wordcloud import WordCloud
```

```{python}
#word cloud about description of listings
description = df.loc[:, 'description']
pattern = re.compile(r'<.*?>')
description_na = description.fillna(' ').values
description_norm = []
for t in description_na:
    clean_text = re.sub(pattern, ' ', t)
    description_norm.append(clean_text)
tfvectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1,3), max_df=0.75, min_df=0.01, stop_words='english')
tfdescription = tfvectorizer.fit_transform(description_norm)
tfdif_des = pd.DataFrame(data=tfdescription.toarray(),columns=tfvectorizer.get_feature_names_out())
tfdif_des.sum().sort_values(ascending=False)
f,ax = plt.subplots(1,1,figsize=(8, 8))
plt.gcf().set_dpi(200)
cloud_des=WordCloud(background_color='white',max_words=120).generate_from_frequencies(tfdif_des.sum())
ax.imshow(cloud_des) 
ax.axis("off")
```

```{python}
#word cloud about host info
host_about = df1.loc[:, 'host_about']
pattern = re.compile(r'<.*?>')
host_about_na = host_about.fillna(' ').values
tfvectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1,3), max_df=0.75, min_df=0.01, stop_words='english')
tfdescription = tfvectorizer.fit_transform(host_about_na)
tfdif_des = pd.DataFrame(data=tfdescription.toarray(),columns=tfvectorizer.get_feature_names_out())
tfdif_des.sum().sort_values(ascending=False)
f,ax = plt.subplots(1,1,figsize=(8, 8))
plt.gcf().set_dpi(200)
cloud_des=WordCloud(background_color='white',max_words=120).generate_from_frequencies(tfdif_des.sum())
ax.imshow(cloud_des) 
ax.axis("off")
```

```{python}
property1 = df.property_type.value_counts()
sorted1 = property1.sort_values(ascending=False).reset_index()
sorted_min = sorted1["count"].min()
sorted_min_type = sorted1[sorted1["count"]==sorted_min]['property_type']
print(f"There is a total of {sorted1.shape[0]} property types, and {sorted1.loc[sorted1['count'].idxmax(), 'property_type']} is the most, which has {sorted1['count'].max()}, {sorted_min_type.tolist()} are the least")
```

```{python}
df1["host_response_rate"] = df1["host_response_rate"].str.replace('%', '').astype(float) / 100
df1["host_acceptance_rate"] = df1["host_acceptance_rate"].str.replace('%', '').astype(float) / 100
response_rate_mean = df1["host_response_rate"].dropna().mean()
acceptance_rate_mean = df1["host_acceptance_rate"].dropna().mean()
print(f"The average acceptance rate of host in London is {acceptance_rate_mean}, and average response rate is {response_rate_mean}")
response_time = df1["host_response_time"].dropna().value_counts()
print(f"{response_time['within an hour']} Landlord replies within an hour, {response_time['within a few hours']} landlord replies within a few hours, {response_time['within a day']} landlord replies within a day, {response_time['a few days or more']} landlord replies only after a few days or more")
```

```{python}
#three aspects about whether hosts have profiles and whether they are superhosts, and whether their identities are verified
identity = df1["host_identity_verified"].dropna().value_counts()
profile = df1["host_has_profile_pic"].dropna().value_counts()
superhost = df1["host_is_superhost"].dropna().value_counts()
f, ax = plt.subplots(figsize=(8,6))
bars1 = identity.plot(position=0, ax=ax, kind="bar", color="grey", width=0.1, label='Counts of T/F Identity Verified')
bars2 = profile.plot(position=1, ax=ax, kind="bar", color="skyblue", width=0.1, label='Counts of T/F Have Profiles')
bars3 = superhost.plot(position=2, ax=ax, kind="bar", color="green", width=0.1, label='Counts of T/F Superhost')
ax.set_ylabel("Counts")
ax.set_xlabel("T/F Identity Verified/Have Profiles/Superhost")
ax.set_xticklabels(['TRUE', 'FALSE'], rotation=45)
plt.legend()
plt.show()
```

```{python}
#the date when the host publish his listing
date = df1["host_since"].dropna()
date = pd.to_datetime(date, format='mixed')
date_df = pd.DataFrame({"dates":date})
year = date_df['dates'].dt.year
year = date_df['dates'].dt.year
year_counts = year.value_counts()
year_counts_df = pd.DataFrame(year_counts).reset_index()
year_counts_df.sort_values(by='dates', ascending=False, inplace=True)
f, ax = plt.subplots(figsize=(8,6))
year_counts_df.plot(kind='bar', ax=ax, x="dates", y="count", color="indigo", legend=False)
ax.set_title("Counts of Host Years")
plt.xticks(rotation=45)
ax.set_xlabel('Year')
ax.set_ylabel('Frequency')
ax.legend()
plt.show()
```

```{python}
# counts of whose listings are more than 100 in inside airbnb data
# only in the scraped data of inside airbnb 
host = df[["host_name", "host_id"]]
host_counts = host.groupby("host_id").size().sort_values(ascending=False).reset_index()
host_counts = host_counts.rename(columns={0:"count"})
host_id = host_counts[host_counts["count"] >= 100]
id_host = host_id["host_id"]
host_filtered = host[host["host_id"].isin(id_host)][["host_name", "host_id"]].drop_duplicates()
print(f"There are {host_filtered.shape[0]} hosts whose listings are more than 100 in Inside Airbnb Data")
total=df1["host_total_listings_count"]
listings_total = total.value_counts().reset_index()
print(f'Landlord with the most properties owns {int(listings_total["host_total_listings_count"].max())} properties, and there are {listings_total["count"].max()} hosts which have one listing')
host_counts_100=host_counts[host_counts["count"]>=100]
host_100 = pd.merge(host_counts_100, host_filtered, on="host_id")
f, ax= plt.subplots(figsize=(8,6))
host_100.plot(ax=ax, kind="bar", x="host_name", y="count", color="turquoise")
ax.set_title("Hosts Whose Listings are More Than 100")
ax.set_xlabel("Name")
ax.set_ylabel("Count")
plt.show()
```

```{python}
verifications_h = df1['host_verifications'].dropna().value_counts().reset_index()
verifications_h.head()
print(f"Most hosts use {verifications_h['host_verifications'].values[0]} as methods of verifications which is as high as {verifications_h['count'].max()}, least hosts use {verifications_h.loc[verifications_h['host_verifications'].idxmin(), 'host_verifications']} as ways of verification which is as low as {verifications_h['count'].min()}")
```

```{python}
max_night = df[df['maximum_nights']<2000]['maximum_nights'].value_counts().reset_index()
max_night.head()
print(f'The most frequent number of maximum nights in these listings is {max_night["maximum_nights"][0]}, which is as high as {max_night["count"].max()}')
```

```{python}
#group by 'property_type','room_type' from practical 9
agg_df = df[(df['price']<=2000) & (df['number_of_reviews']>0)][['price', 'property_type','room_type','number_of_reviews']]
agg = agg_df.groupby(by=['property_type','room_type']).agg(counts = ("property_type", "count"), median_price = ("price", "median")).reset_index().sort_values(by=['counts','room_type','property_type'], ascending=[False,True,True])
agg.head(20)
```

```{python}
# Unless planning permission is obtained, Londoners are restricted to renting their property short term for a maximum of 90 nights in a calendar year.
# from https://www.london.gov.uk/programmes-strategies/housing-and-land/improving-private-rented-sector/guidance-short-term-and-holiday-lets-london
# Next is STL-related analysis
bnb_stl = bnb_gdf[bnb_gdf['maximum_nights']<=90][['price', 'room_type', 'property_type', 'maximum_nights', 'geometry', 'number_of_reviews']]
stl_borough=gpd.sjoin(bnb_stl, boroughs, how="inner", predicate="within")
stl_counts=stl_borough.groupby("NAME").size().reset_index()
stl_counts=stl_counts.rename(columns={0:"counts"})
stl_merge=pd.merge(boroughs, stl_counts, on="NAME")
f, ax=plt.subplots(figsize=(12,8))
ax=stl_merge.plot(ax=ax, column="counts", cmap="viridis", edgecolor="black", legend=True)
plt.title("Airbnb STL Listings Distribution in Boroughs")
plt.show()
```

```{python}
from bokeh.plotting import figure
from bokeh.io import output_file, show, output_notebook, push_notebook, export_png 
from bokeh.models import ColumnDataSource, GeoJSONDataSource, LinearColorMapper, ColorBar, HoverTool
from bokeh.palettes import brewer
import json
bnb_stl = bnb_gdf[bnb_gdf['maximum_nights']<=90][['price', 'room_type', 'property_type', 'maximum_nights', 'geometry','number_of_reviews']]
stl_borough = gpd.sjoin(bnb_stl, boroughs, how="inner", predicate="within")
stl_counts = stl_borough.groupby("NAME").agg(count=('NAME', 'count'), maximum=('maximum_nights', lambda x: x.mode()[0]), price=('price', 'median')).reset_index()
stl_merge1 =pd.merge(boroughs, stl_counts, on="NAME")

def get_geodata(gdf):    
    json_data = json.dumps(json.loads(gdf.to_json()))
    return GeoJSONDataSource(geojson = json_data)
stlgeo = stl_merge1.to_crs('epsg:3785')
column = 'count'
geo_source = get_geodata(stlgeo)
palette=brewer['YlGnBu'][8]
cols = stlgeo[column]
color_mapper = LinearColorMapper(palette=palette, low=cols.min(), high=cols.max()) 
color_bar = ColorBar(color_mapper=color_mapper, label_standoff=10, width=500, height=8, location=(0,0), orientation='horizontal')
tools = 'wheel_zoom,pan,reset,hover'
p2 = figure(title = "Borough-level STL Listings", height=700, width=850, toolbar_location='right', tools=tools) 
p2.add_tile("CartoDB Positron", retina=True)  
p2.xgrid.grid_line_color = None 
p2.ygrid.grid_line_color = None
p2.patches('xs','ys', source=geo_source, fill_alpha=0.5, line_width=0.5, line_color='white', fill_color={'field' :column , 'transform': color_mapper}) 
p2.add_layout(color_bar, 'below') 
hover = HoverTool()
hover.point_policy = "follow_mouse" 
hover.tooltips = [("Borough", "@NAME"), ("Maximum Nights", "@maximum"), ("Count of Listings", "@count"), ("Median Price", "$@price")]
p2.add_tools(hover)
handle = show(p2, notebook_handle=True)
push_notebook(handle=handle)
output_notebook() 
```

```{python}
stl = df[df['maximum_nights']<=90][['price', 'room_type', 'property_type', 'maximum_nights']]
stl_agg = stl.groupby(by=['property_type','room_type']).agg(counts = ("property_type", "count"), mean_price = ("price", "mean")).reset_index().sort_values(by=['counts','room_type','property_type'], ascending=[False,True,True])
stl_agg.head()
```

```{python}
stl_rtype = stl['room_type'].value_counts()
stl_rtype.head()
stl_ptype = stl['property_type'].value_counts().reset_index()
stl_ptype_7 = stl_ptype[:7]
other_types = pd.DataFrame({"property_type":['Others'], "count":[stl_ptype['count'][7:].sum()]})
stl_ptype2 = pd.concat([stl_ptype_7, other_types], ignore_index=True)
plt.figure(figsize=(8,6))
plt.pie(stl_ptype2['count'], labels=stl_ptype2['property_type'], autopct='%1.1f%%', startangle=90)
plt.title("STL Property Type Pie Chart")
plt.show()
```

```{python}
stl_price = stl[stl['price']<=1000]['price']
f,ax = plt.subplots(figsize=(8, 6))
ax.hist(stl_price, bins=20, edgecolor="black", color="red")
ax.set_xlabel("Price")
ax.set_ylabel("Frequency")
ax.set_title("Price Distribution")
plt.show()
```

```{python}
stl_sh = df[df['maximum_nights']<=90][['host_is_superhost', 'host_total_listings_count']]
stl_sh_count = stl_sh.groupby('host_is_superhost').agg(count_sh=('host_is_superhost', 'count'), mean_count=('host_total_listings_count', 'mean')).reset_index()
print(stl_sh_count.head())
print(f'There are a total of listings of {stl.shape[0]} STL listings in London')
```

```{python}
#read msoa-level dara using function
MSOA = gpd.read_file(data_cache('http://orca.casa.ucl.ac.uk/~jreades/data/MSOA-2011.gpkg', dest))
MSOA = MSOA.to_crs(epsg=27700)
MSOA['area_km2'] = MSOA['geometry'].area/1e6
```

```{python}
#msoa-level msoa 
bnb_stl = bnb_gdf[bnb_gdf['maximum_nights']<=90][['price', 'room_type', 'property_type', 'maximum_nights', 'geometry', 'number_of_reviews']]
stl_msoa = gpd.sjoin(bnb_stl, MSOA.drop(columns=['MSOA11NM', 'LAD11CD', 'LAD11NM', 'RGN11CD', 'RGN11NM','USUALRES', 'HHOLDRES', 'COMESTRES', 'POPDEN', 'HHOLDS', 'AVHHOLDSZ']), how="inner", predicate="within")
stl_counts_msoa = stl_msoa.groupby("MSOA11CD").size().reset_index()
stl_counts_msoa = stl_counts_msoa.rename(columns={0:"counts"})
stl_merge_msoa = pd.merge(MSOA.drop(columns=['MSOA11NM', 'LAD11CD', 'LAD11NM', 'RGN11CD', 'RGN11NM','USUALRES', 'HHOLDRES', 'COMESTRES', 'POPDEN', 'HHOLDS', 'AVHHOLDSZ']), stl_counts_msoa, on="MSOA11CD")
f, ax = plt.subplots(figsize=(12,8))
ax = stl_merge_msoa.plot(ax=ax, column="counts", cmap="viridis", edgecolor="black", legend=True)
plt.title("Airbnb STL Listings Distribution in Boroughs")
plt.show()
```

```{python}
from sklearn.neighbors import NearestNeighbors
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, PowerTransformer
from sklearn.cluster import KMeans, DBSCAN 
from sklearn.metrics import silhouette_samples, silhouette_score

pivot_stl = stl_borough.groupby(['GSS_CODE','room_type'], observed=False)['index_right'].agg(Count='count').reset_index().pivot(index='GSS_CODE', columns='room_type', values='Count')         
pivot_stl.drop(['Hotel room', 'Shared room'], axis=1, inplace=True)
pivot_norm = pd.DataFrame(index=pivot_stl.index)
for cols in pivot_stl.columns:
    pivot_norm[cols]=PowerTransformer().fit_transform(pivot_stl[cols].to_numpy().reshape(-1, 1))
clu_median = stl_borough.groupby(by='GSS_CODE').price.agg('median')
pivot_norm["median price"] = pd.Series(np.squeeze(RobustScaler().fit_transform(clu_median.values.reshape(-1,1))), index=clu_median.index) 
reviews_mean = stl_borough.groupby(by='GSS_CODE')["number_of_reviews"].agg('mean')
pivot_norm["mean reviews"] = pd.Series(np.squeeze(MinMaxScaler().fit_transform(reviews_mean.values.reshape(-1,1))), index=reviews_mean.index) 
#find suitable k
#k_range = []
#scores = []
#for k in range(2,32):
    #kmeans = KMeans(n_clusters=k, n_init=25, random_state=42).fit(pivot_norm)
    #ave_score = silhouette_score(pivot_norm, kmeans.labels_)
    #k_range.append(k)
    #scores.append(ave_score)
#plt.plot(k_range, scores)
#plt.gcf().suptitle("Average Silhouette Scores");
n = 5
colors = ['#8B4513', '#D2691E', '#DAA520', '#CD853F', '#A0522D']
kmeans = KMeans(n_clusters=n, n_init=25, random_state=42).fit(pivot_norm)
pivot_norm['cluster']=kmeans.labels_
result_clu_stl = boroughs.merge(pivot_norm, on="GSS_CODE")
f, ax = plt.subplots(figsize=(12,8))
result_clu_stl.plot(ax=ax, column="cluster", cmap=matplotlib.colors.ListedColormap(colors), edgecolor="black")
plt.title("STL Cluster By London Boroughs")
plt.show()
```

```{python}
bnb_1 = bnb_gdf[['price', 'room_type', 'property_type', 'maximum_nights', 'geometry', 'number_of_reviews']]
msoa_listings = gpd.sjoin(bnb_1, MSOA.drop(columns=['MSOA11NM', 'LAD11CD', 'LAD11NM', 'RGN11CD', 'RGN11NM','USUALRES', 'HHOLDRES', 'COMESTRES', 'POPDEN', 'HHOLDS', 'AVHHOLDSZ']), how="inner", predicate="within")
count_msoa = msoa_listings.groupby('MSOA11CD')['index_right'].agg('count').reset_index()
count_msoa['area_km2'] = count_msoa['MSOA11CD'].map(msoa_listings.groupby('MSOA11CD')['area_km2'].first())
count_msoa.rename(columns={'index_right': 'Count'}, inplace=True)
count_msoa.set_index('MSOA11CD', inplace=True)
count_msoa["density"]=round(count_msoa["Count"]/count_msoa["area_km2"],2)
count_msoa.drop(['Count', 'area_km2'], axis=1, inplace=True)
count_msoa['density'] = PowerTransformer().fit_transform(count_msoa['density'].to_numpy().reshape(-1, 1))
price_msoa = msoa_listings.groupby(by='MSOA11CD').price.agg('median')
count_msoa["median price"] = pd.Series(np.squeeze(RobustScaler().fit_transform(price_msoa.values.reshape(-1,1))), index=price_msoa.index) 
reviews_msoa = msoa_listings.groupby(by='MSOA11CD')["number_of_reviews"].agg('count')
count_msoa["mean reviews"] = pd.Series(np.squeeze(MinMaxScaler().fit_transform(reviews_msoa.values.reshape(-1,1))), index=reviews_msoa.index) 
#find suitable k
k_range = [] 
scores = []
for k in range(2,30):
    kmeans = KMeans(n_clusters=k, n_init=25, random_state=42).fit(count_msoa)
    ave_score = silhouette_score(count_msoa, kmeans.labels_)
    k_range.append(k)
    scores.append(ave_score)
plt.plot(k_range, scores)
plt.gcf().suptitle("Average Silhouette Scores");
```

```{python}
from kneed import KneeLocator
db_name = "DBSCAN"
nearest_nei = NearestNeighbors(n_neighbors=6).fit(count_msoa)
neigh_distance, _ = nearest_nei.kneighbors(count_msoa)  
neigh_distance = np.sort(neigh_distance, axis=0)
neigh_distance = neigh_distance[:, 1]
kn = KneeLocator(np.arange(neigh_distance.shape[0]), neigh_distance, S=12, curve='convex', direction='increasing')
kn.plot_knee()
kn.plot_knee_normalized()
epsilon_knee = neigh_distance[kn.knee]
print(f'the best epsilon is {epsilon_knee}')
dbscan = DBSCAN(eps=epsilon_knee, min_samples=count_msoa.shape[1]+1).fit(count_msoa.values)
clu_name = pd.Series(dbscan.labels_, index=count_msoa.index, name=db_name)
count_msoa[db_name] = clu_name
cols = ['density', 'median price', 'mean reviews']
centroid_mean = pd.DataFrame(columns=cols)
for k in sorted(count_msoa[db_name].unique()):
    print(f"Processing cluster {k}")
    cluster = count_msoa[count_msoa[db_name]==k]  
    centroid_mean.loc[k] = cluster[cols].mean()  
centroid_mean.drop(labels=[-1], axis=0, inplace=True) MSOA = gpd.read_file(data_cache('http://orca.casa.ucl.ac.uk/~jreades/data/MSOA-2011.gpkg', dest))
```

```{python}
#read parquet, do one-hot encode
msoa_atlas = pd.read_parquet("https://github.com/BiuLei0527/CASA0013_groupwork/raw/refs/heads/main/MSOA_Atlas.parquet")
from sklearn.preprocessing import OneHotEncoder
from umap import UMAP
msoa_atlas.set_index("MSOA Code", inplace=True)
boro_enco = OneHotEncoder(sparse_output=False)
sub_enco = OneHotEncoder(sparse_output=False)
encoded_borough = boro_enco.fit_transform(msoa_atlas[['Borough']])
encoded_subregion = sub_enco.fit_transform(msoa_atlas[['Subregion']])
encoded_boro = pd.DataFrame(encoded_borough, columns=boro_enco.get_feature_names_out(['Borough']),index=msoa_atlas.index)
encoded_sub = pd.DataFrame(encoded_subregion, columns=sub_enco.get_feature_names_out(['Subregion']),index=msoa_atlas.index)
```

```{python}
#choose cluster variables
cluster = msoa_atlas.drop(['MSOA Name', 'Borough', 'Subregion',
    "Age-All Ages", "Households-All Households", 
    "Economic Activity-Economically inactive: Total", 
    "Economic Activity-Economically active: Total", 
    "Total Median hh Income", "Detached", "Semi-detached",
    "Terraced (including end-terrace)", "Flat, maisonette or apartment", 
    "Population Density-Persons per hectare (2012)", 
    "Qualifications-Other", "Vehicles-Cars per hh", 
    "BAME", "Language-1+ English as a main language", 
    "Language-None have English as main language", 'Vehicles-Sum of all cars or vans in the area'], axis=1)
cluster.set_index("MSOA Code", inplace=True)
cluster = pd.concat([cluster, encoded_boro, encoded_sub], axis=1)
```

```{python}
#do standardising
scaler_data = cluster.loc[:, 'Age-0-15':'Vehicles-4 or more cars or vans in hh'] 
scaler = RobustScaler().fit_transform(scaler_data)
cluster.loc[:, 'Age-0-15':'Vehicles-4 or more cars or vans in hh'] = scaler
```

```{python}
#reduce dimensionality using umap
dimensions=3 
umap = UMAP(n_neighbors=25, min_dist=0.05, n_components=dimensions, random_state=42)
umap_clu = umap.fit_transform(cluster)
print(umap_clu.shape)
```


